{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import wandb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {device}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"my-awesome-project\",\n",
    "\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"CNN\",\n",
    "    \"epochs\": 10,\n",
    "    \"loss\": nn.BCELoss(),\n",
    "    \"batch_size\": 64\n",
    "    }\n",
    ")\n",
    "config = wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def log_image_table(images, predicted, labels, probs):\n",
    "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
    "    # üêù Create a wandb Table to log images, labels and predictions to\n",
    "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\n",
    "    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\n",
    "        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\n",
    "    wandb.log({\"predictions_table\":table}, commit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loading(data_path, label_path, batch_size, train_ratio=0.8):\n",
    "    data = torch.load(data_path)\n",
    "    labels = torch.load(label_path)\n",
    "    data = nn.functional.normalize(data, dim=1)\n",
    "    \n",
    "    # reshaping the dataset so  the channels will be the second dimension.\n",
    "    data = data.permute(0, 2, 1)\n",
    "    n_samples, n_channels, n_features = data.shape\n",
    "    print(type(data))\n",
    "    X_reshaped = data.reshape(n_samples, -1) \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    data_resampled, labels_resampled = smote.fit_resample(X_reshaped, labels)\n",
    "\n",
    "    # Reshape X_resampled back to 3D\n",
    "    data_resampled = data_resampled.reshape(-1, n_channels, n_features)\n",
    "    print(type(labels_resampled))\n",
    "    print(data_resampled.shape)\n",
    "    dataset = TensorDataset(data_resampled, labels_resampled)\n",
    "    train_size = int(train_ratio * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    return DataLoader(train_dataset, batch_size=batch_size, shuffle=True), DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "train_loader, test_loader = data_loading(data_path='data.pt', label_path='label.pt',batch_size= batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader)) # Requests the first training batch\n",
    "print(X.size()) \n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, batch_size):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=24, out_channels=32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=32, kernel_size=5)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, padding=0, stride=1)\n",
    "        # the size of the out channels x number  of nodes \n",
    "        self.fc1 = nn.Linear(32*91, 200)\n",
    "        self.fc2 = nn.Linear(200, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x - self.dropout(x)\n",
    "        x = self.sigmoid(self.fc2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the CNN\n",
    "model = SimpleCNN(batch_size)\n",
    "lr = 0.004\n",
    "# Loss and optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct(logits, y):\n",
    "    y_hat = logits.argmax(axis=1) # Finds the column with the highest value for each row of `logits`.\n",
    "    return (y_hat == y).float().sum() # Computes the number of times that `y_hat` and `y` match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metric(model, data_iter, metric):\n",
    "    \"\"\"Compute the average `metric` of the model on a dataset.\"\"\"\n",
    "    c = torch.tensor(0.).to(device)\n",
    "    n = torch.tensor(0.).to(device)\n",
    "    for X, y in data_iter:\n",
    "        X, y = X.to(device), y.to(device) # Moves data to `device`\n",
    "        logits = model(X)\n",
    "        c += metric(logits, y)\n",
    "        n += len(y)\n",
    "    return c*100 / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [] # Stores the loss for each training batch\n",
    "train_accs = [] # Stores the training accuracy after each epoch\n",
    "test_accs = [] # Stores the testing accuracy after each epoch\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{num_epochs}.')\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    model.train() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device) # Moves data to `device`\n",
    "        logits = model(X) # Computes the logits for the batch of images `X`\n",
    "        l = loss(logits, y) # Computes the loss given the `logits` and the class vector `y`\n",
    "        optimizer.zero_grad() # Zeroes the gradients stored in the model parameters\n",
    "        l.backward() # Computes the gradient of the loss `l` with respect to the model parameters\n",
    "\n",
    "        optimizer.step() # Updates the model parameters based on the gradients stored inside them\n",
    "\n",
    "        losses.append(float(l)) # Stores the loss for this batch\n",
    "\n",
    "    model.eval() # This is necessary because batch normalization behaves differently between training and evaluation\n",
    "    train_accs.append(evaluate_metric(model, train_loader, correct))\n",
    "    test_accs.append(evaluate_metric(model, test_loader, correct))\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "\n",
    "    print(f'Training accuracy: {train_accs[-1]}. Testing accuracy: {test_accs[-1]}. Duration: {end_time - start_time:.3f}s.') # Computes and displays training/testing dataset accuracy.\n",
    "\n",
    "plt.plot(losses) # Plots the loss for each training batch\n",
    "plt.xlabel('Training batch')\n",
    "plt.ylabel('Cross entropy loss')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(list(map(lambda x: x.cpu(),train_accs)), label='Training accuracy')\n",
    "plt.plot(list(map(lambda x: x.cpu(),test_accs)), label='Testing accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move data to the appropriate device (CPU or GPU)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            predicted = outputs.argmax(axis=1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Loss: {test_loss / len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer, epochs=100)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate(model, test_loader, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
